{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuPFow4NiiZx",
        "outputId": "35b63aea-22ee-4db7-d3d4-241546866b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Input: 1\n",
            "Threshold:  1\n",
            "Sigmoid:  0.7310585786300049\n",
            "Relu:  1\n",
            "Tanh:  0.7615941559557649\n",
            "Leaky Relu:  1\n"
          ]
        }
      ],
      "source": [
        "# Demonstration of Activation Functions\n",
        "\n",
        "import math\n",
        "\n",
        "def threshold(x):               # Step Function or Threshold Function\n",
        "  op = 1 if x >= 0 else 0\n",
        "  return op\n",
        "\n",
        "def sigmoid(x):                 # Sigmoid Function\n",
        "  op = 1 / (1 + math.exp(-x))\n",
        "  return op\n",
        "\n",
        "def relu(x):                    # Rectified Linear Unit Function\n",
        "  op = max(0,x)\n",
        "  return op\n",
        "\n",
        "def tanh(x):                    # Hyperbolic Tangent Function\n",
        "  op = (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))\n",
        "  return op\n",
        "\n",
        "def leaky_relu(x):              # Leaky Rectified Linear Unit Function\n",
        "  op = max(0.1*x, x)\n",
        "  return op\n",
        "\n",
        "x = int(input(\"Enter Input: \"))\n",
        "print(\"Threshold: \", threshold(x))\n",
        "print(\"Sigmoid: \", sigmoid(x))\n",
        "print(\"Relu: \", relu(x))\n",
        "print(\"Tanh: \", tanh(x))\n",
        "print(\"Leaky Relu: \", leaky_relu(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and Implement Basic Neuron Model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, num_inputs):\n",
        "    self.weights = np.random.rand(num_inputs)\n",
        "    self.bias = np.random.rand()\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def activate(self, inputs):\n",
        "    weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
        "    output = self.sigmoid(weighted_sum)\n",
        "    return output\n",
        "\n",
        "neuron = Neuron(3)\n",
        "inputs = np.array([0.5, 0.2, 0.8])\n",
        "output = neuron.activate(inputs)\n",
        "\n",
        "print(\"Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXYw6b91qfRB",
        "outputId": "e25d1da4-3185-4966-b4e4-c93e1e7d6fe0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: 0.6007309977485483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of XOR Network\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def activate(self, inputs):\n",
        "    weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
        "    output = 1 / (1 + np.exp(-weighted_sum))\n",
        "    return output\n",
        "\n",
        "def xor_network(input1, input2):\n",
        "\n",
        "  h1 = Neuron(np.array([2, 2]), -3)\n",
        "  h2 = Neuron(np.array([-2, -2]), 1)\n",
        "\n",
        "  output_neuron = Neuron(np.array([2, 2]), -1)\n",
        "\n",
        "  hidden_output1 = h1.activate(np.array([input1, input2]))\n",
        "  hidden_output2 = h2.activate(np.array([input1, input2]))\n",
        "\n",
        "  final_output = output_neuron.activate(np.array([hidden_output1, hidden_output2]))\n",
        "  return final_output\n",
        "\n",
        "\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "for input in inputs:\n",
        "  print(f\"Input: {input}, Output: {xor_network(input[0], input[1])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMK90jAwupHL",
        "outputId": "182f1fa6-1867-425c-b07b-46da099af32d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0, 0], Output: 0.6357509134091353\n",
            "Input: [0, 1], Output: 0.5189323655620768\n",
            "Input: [1, 0], Output: 0.5189323655620768\n",
            "Input: [1, 1], Output: 0.6357509134091353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of XOR Network where Hidden Layer has RELU and Output Layer has Sigmoid\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias, activation_function):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "    self.activation_function = activation_function\n",
        "\n",
        "  def activate(self, inputs):\n",
        "    weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
        "    output = self.activation_function(weighted_sum)\n",
        "    return output\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "  return max(0, x)\n",
        "\n",
        "def xor_network(input1, input2):\n",
        "\n",
        "  h1 = Neuron(np.array([1, 1]), -1, relu)\n",
        "  h2 = Neuron(np.array([-1, 1]), 0, relu)\n",
        "\n",
        "  output_neuron = Neuron(np.array([1, -1]), 1, sigmoid)\n",
        "\n",
        "  hidden_output1 = h1.activate(np.array([input1, input2]))\n",
        "  hidden_output2 = h2.activate(np.array([input1, input2]))\n",
        "\n",
        "  final_output = output_neuron.activate(np.array([hidden_output1, hidden_output2]))\n",
        "  return final_output\n",
        "\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "for input in inputs:\n",
        "  print(f\"Input: {input}, Output: {xor_network(input[0], input[1])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57OpGpGrw101",
        "outputId": "6f3433fb-56c6-4241-b136-cbd986cb1759"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0, 0], Output: 0.7310585786300049\n",
            "Input: [0, 1], Output: 0.5\n",
            "Input: [1, 0], Output: 0.7310585786300049\n",
            "Input: [1, 1], Output: 0.8807970779778823\n"
          ]
        }
      ]
    }
  ]
}